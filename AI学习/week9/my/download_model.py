from modelscope import snapshot_download, AutoTokenizer
from transformers import AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq
import torch
import os
from mlx_lm import load , generate

# è·å–è„šæœ¬æ‰€åœ¨ç›®å½•ï¼Œå¹¶åˆ›å»ºæ¨¡å‹ç¼“å­˜è·¯å¾„
script_path = os.path.dirname(os.path.abspath(__file__))
cache_path = os.path.join(script_path, "../models")

# åœ¨modelscopeä¸Šä¸‹è½½Qwenæ¨¡å‹åˆ°æœ¬åœ°ç›®å½•ä¸‹
# model_dir = snapshot_download("Qwen/Qwen3-0.6B", cache_dir=cache_path, revision="master")
# # TransformersåŠ è½½æ¨¡å‹æƒé‡
# tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=False, trust_remote_code=True)
# model = AutoModelForCausalLM.from_pretrained(model_dir, device_map="auto", torch_dtype=torch.bfloat16)

# todo macç”µè„‘å•ç‹¬æ”¯æŒçš„MLXç‰ˆæœ¬. ä½¿ç”¨æ–¹å¼ä¸é€šç”¨ç‰ˆæœ¬çš„ä¸åŒ.ä¸æ”¯æŒpytorch å’Œ transformers
# model_dir = snapshot_download("Qwen/Qwen3-1.7B-MLX-8bit", cache_dir=cache_path, revision="master")
model, tokenizer = load(
    "../models/Qwen/Qwen3-1___7B-MLX-8bit"  # æ˜ç¡®æŒ‡å®šä½¿ç”¨Appleçš„GPUåŠ é€Ÿ
)
prompt = "è¯·ä»‹ç»ä¸€ä¸‹æœºå™¨å­¦ä¹ "
response = generate(
    model=model,
    tokenizer=tokenizer,
    prompt=prompt,
    max_tokens=150
)

print("æ¨¡å‹è¾“å‡º:", response)

# while True:
#     try:
#         question = input("ğŸ‘¤ è¯·è¾“å…¥æ‚¨çš„é—®é¢˜: ").strip()
#
#         if question.lower() in ['quit', 'exit', 'é€€å‡º']:
#             print("ğŸ‘‹ å†è§ï¼")
#             break
#
#         if not question:
#             continue
#
#         response = generate(
#             model=model,
#             tokenizer=tokenizer,
#             prompt=question,
#             max_tokens=150
#         )
#
#         print(f"ğŸ¤– {response}")
#
#
#     except KeyboardInterrupt:
#         print("\nğŸ‘‹ å†è§ï¼")
#         break
#     except Exception as e:
#         print(f"âŒ å‡ºç°é”™è¯¯: {e}")