import torch
from sympy.physics.units import temperature
from torch.optim import AdamW
from torchgen.api.cpp import return_type
from transformers import AutoTokenizer,BertModel,GPT2Model
from transformers import BertForSequenceClassification , GPT2LMHeadModel
import warnings
warnings.filterwarnings("ignore")

class BERTGPTDEMO:
    def __init__(self):
        self.device = torch.device('mps' if torch.mps.is_available() else 'cpu')
        print(f"ä½¿ç”¨è®¾å¤‡:{self.device}")

    def load_models(self):
        self.bert_tokenizer = AutoTokenizer.from_pretrained("bert-base-chinese")
        self.bert_model = BertModel.from_pretrained("bert-base-chinese").to(self.device)
        self.bert_classifier = BertForSequenceClassification.from_pretrained("bert-base-chinese"
                                                                             ,num_labels = 2).to(self.device)
        self.gpt_tokenizer = AutoTokenizer.from_pretrained("gpt2")
        self.gpt_base = GPT2Model.from_pretrained("gpt2").to(self.device)
        self.gpt_model = GPT2LMHeadModel.from_pretrained("gpt2").to(self.device)
        self.gpt_tokenizer.pad_token = self.gpt_tokenizer.eos_token

        print("æ¨¡å‹åŠ è½½å®Œæˆ")


    def compare_architectures(self):
        """1. æ¶æ„å¯¹æ¯”åˆ†æ"""
        print("\n=== 1. æ¶æ„å¯¹æ¯”åˆ†æ ===")

        architecture_comparison = {
            "æ ¸å¿ƒæ¶æ„": {
                "BERT": "åŒå‘Transformerç¼–ç å™¨",
                "GPT": "å•å‘Transformerè§£ç å™¨"
            },
            "æ³¨æ„åŠ›æœºåˆ¶": {
                "BERT": "å…¨è¯æ³¨æ„åŠ›ï¼ˆæ— æ©ç ï¼‰",
                "GPT": "å› æœæ©ç è‡ªæ³¨æ„åŠ›"
            },
            "è¾“å…¥å¤„ç†": {
                "BERT": "[CLS] + å¥å­å¯¹",
                "GPT": "åºåˆ—è‡ªå›å½’"
            },
            "è¾“å‡ºæ–¹å¼": {
                "BERT": "ä¸Šä¸‹æ–‡è¡¨å¾å‘é‡",
                "GPT": "ä¸‹ä¸€ä¸ªtokené¢„æµ‹"
            }
        }

        print("æ¶æ„å¯¹æ¯”:")
        for aspect, models in architecture_comparison.items():
            print(f"\n{aspect}:")
            print(f"  BERT: {models['BERT']}")
            print(f"  GPT: {models['GPT']}")

    def compare_pretraining_tasks(self):
        """2. é¢„è®­ç»ƒä»»åŠ¡å¯¹æ¯”"""
        print("\n=== 2. é¢„è®­ç»ƒä»»åŠ¡å¯¹æ¯” ===")

        task_comparison = {
            "ä¸»è¦ä»»åŠ¡": {
                "BERT": "MLMï¼ˆæ©ç è¯­è¨€å»ºæ¨¡ï¼‰+ NSPï¼ˆä¸‹ä¸€å¥é¢„æµ‹ï¼‰",
                "GPT": "è‡ªå›å½’è¯­è¨€å»ºæ¨¡ï¼ˆé¢„æµ‹ä¸‹ä¸€è¯ï¼‰"
            },
            "è®­ç»ƒç›®æ ‡": {
                "BERT": "å­¦ä¹ åŒå‘ä¸Šä¸‹æ–‡è¡¨å¾",
                "GPT": "å­¦ä¹ ç”Ÿæˆæ¦‚ç‡åˆ†å¸ƒ"
            },
            "æ•°æ®åˆ©ç”¨": {
                "BERT": "åˆ©ç”¨å·¦å³ä¸Šä¸‹æ–‡",
                "GPT": "åˆ©ç”¨å‰åºä¸Šä¸‹æ–‡"
            },
            "æ³›åŒ–èƒ½åŠ›": {
                "BERT": "å¼ºç†è§£èƒ½åŠ›",
                "GPT": "å¼ºç”Ÿæˆèƒ½åŠ›"
            }
        }

        print("é¢„è®­ç»ƒä»»åŠ¡å¯¹æ¯”:")
        for aspect, models in task_comparison.items():
            print(f"\n{aspect}:")
            print(f"  BERT: {models['BERT']}")
            print(f"  GPT: {models['GPT']}")

    def bert_bidirectional(self):
        print("\n=== 3. BERTåˆ†è¯æ¼”ç¤º ===")
        text = "ä¸­å›½çš„é¦–éƒ½åŒ—äº¬æ˜¯ä¸€ä¸ªç°ä»£åŒ–å¤§éƒ½å¸‚"

        tokens = self.bert_tokenizer.tokenize(text)
        print(f"åˆ†è¯ç»“æœ: {tokens}")

        inputIds = self.bert_tokenizer.encode(text,add_special_tokens=True)
        print(f"è¾“å…¥ID: {inputIds}")
        print(f"ç‰¹æ®Š[CLS]:{self.bert_tokenizer.cls_token_id},ç‰¹æ®Š[SEP]:{self.bert_tokenizer.sep_token_id}")

    def bert_demonstrate_tokenization(self):
        print("\n=== 4. BERTåˆ†è¯æ¼”ç¤º ===")

        text = "ä¸­å›½çš„é¦–éƒ½åŒ—äº¬æ˜¯ä¸€ä¸ªç°ä»£åŒ–å¤§éƒ½å¸‚"
        mask_text= "ä¸­å›½çš„é¦–éƒ½[MASK]æ˜¯ä¸€ä¸ªç°ä»£åŒ–å¤§éƒ½å¸‚"

        print(f"åŸå§‹æ–‡æœ¬: {text}")
        print(f"æ©ç æ–‡æœ¬: {mask_text}")

        # è½¬æ¢ä¸ºID
        input_ids = self.bert_tokenizer(mask_text,return_tensors="pt").to(self.device)
        with torch.no_grad():
            outputs = self.bert_classifier(**input_ids)
            logits = outputs.logits
            print(f"outputs: {outputs}")
            print(f"logits: {logits}")


    def bert_attention_mask(self):
        print("\n=== 5. BERTçš„æ³¨æ„åŠ›æ©ç  ===")
        text1 = "é¦–éƒ½åŒ—äº¬æ˜¯ä¸€ä¸ªç°ä»£åŒ–å¤§éƒ½å¸‚"
        text2 = "é¦–éƒ½åŒ—äº¬ç‰¹åˆ«ç¾"
        texts = [text1,text2]
        encode = self.bert_tokenizer(texts,padding=True,truncation=True,return_tensors="pt")
        print(f"è¾“å…¥Id: {encode['input_ids'].shape}")
        print(f"æ³¨æ„åŠ›æ©ç : {encode['attention_mask']}")

    def bert_forward(self):
        print("\n=== 6. BERTå‰å‘ä¼ æ’­æ¼”ç¤º ===")
        text = "ä¸­å›½çš„é¦–éƒ½åŒ—äº¬æ˜¯ä¸€ä¸ªç°ä»£åŒ–å¤§éƒ½å¸‚"
        encode = self.bert_tokenizer(text,return_tensors="pt").to(self.device)
        with torch.no_grad():
            outputs = self.bert_model(**encode)
        print(f"last_hidden_state: {outputs.last_hidden_state.shape}")
        print(f"pooler_output: {outputs.pooler_output.shape}")

        cls_embedding = outputs.last_hidden_state[0,0,:]
        print(f"cls_embedding: {cls_embedding.shape}")

    def gpt_unidirectional(self):
        print("\n=== 7. GPTè‡ªå›å½’è¯­è¨€å»ºæ¨¡æ¼”ç¤º ===")
        text = "This evening eat"
        print(f"è¾“å…¥æ–‡æœ¬: {text}")

        inputs = self.gpt_tokenizer(text,return_tensors="pt").to(self.device)
        with torch.no_grad():
            outputs = self.gpt_model(**inputs)
            next_token_logits = outputs.logits[:, -1, :]
            print(f"next_token_logits: {next_token_logits.shape}")

        top_k = 5
        top_tokens = torch.topk(next_token_logits,top_k,dim=-1)

        for i in range(top_k):
            token_id = top_tokens.indices[0,i].item()
            token = self.gpt_tokenizer.decode(token_id)
            prob = torch.softmax(next_token_logits,dim=-1)[0,token_id].item()
            print(f"é¢„æµ‹ä¸‹ä¸€ä¸ªtoken: {token} (æ¦‚ç‡: {prob:.4f})")

    def gpt_fine_tune_generator(self):
        print("\n=== 8. GPTå¾®è°ƒæ¼”ç¤º ===")
        prompts = self.create_generation_dataset()

        optimizer = AdamW(self.gpt_model.parameters(),lr=5e-5)
        num_epochs = 5

        for epoch in range(num_epochs):
            total_loss = 0
            for prompt in prompts:
                inputs = self.gpt_tokenizer(prompt,padding=True,truncation=True,return_tensors="pt").to(self.device)

                labels = inputs['input_ids'].clone()
                outputs = self.gpt_model(**inputs,labels=labels)
                loss = outputs.loss

                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                total_loss +=loss.item()

            avg_loss = total_loss / len(prompts)
            print(f"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}")

        print("å¾®è°ƒå®Œæˆ")

        test_prompt = "Today is the weekend. What should I do, "
        test_inputs = self.gpt_tokenizer(test_prompt,return_tensors="pt").to(self.device)

        with torch.no_grad():
            test_outputs = self.gpt_model.generate(
                test_inputs['input_ids'],
                max_length=50,
                temperature = 0.8,
                do_sample=True,
                pad_token_id=self.gpt_tokenizer.eos_token_id
            )

        generated_text = self.gpt_tokenizer.decode(test_outputs[0],skip_special_tokens=True)
        print(f"ç”Ÿæˆæ–‡æœ¬: {generated_text}")


    def create_generation_dataset(self):
        """åˆ›å»ºæ–‡æœ¬ç”Ÿæˆæ•°æ®é›†"""
        # æ¨¡æ‹Ÿæ•…äº‹ç»­å†™æ•°æ®é›†
        prompts = [
            "Once upon a time, in a magical forest,",
            "The scientist discovered a new element that",
            "In the year 2050, artificial intelligence",
            "The young adventurer found an ancient map leading to"
        ]
        return prompts

    def run_all_demos(self):
        print("ğŸš€ å¼€å§‹ä½œä¸šæ¼”ç¤º\n")

        self.load_models()
        self.compare_architectures()
        self.compare_pretraining_tasks()
        self.bert_bidirectional()
        self.bert_demonstrate_tokenization()
        self.bert_attention_mask()
        self.bert_forward()
        self.gpt_unidirectional()
        self.gpt_fine_tune_generator()

        print("\nğŸ‰ ä½œä¸šå®Œæˆï¼")

# ä¸»ç¨‹åº
if __name__ == "__main__":
    demo = BERTGPTDEMO()
    demo.run_all_demos()
