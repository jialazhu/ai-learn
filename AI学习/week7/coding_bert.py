import torch
import torch.nn as nn
from transformers import AutoTokenizer, BertModel, BertForMaskedLM, BertForSequenceClassification
from torch.optim import AdamW
import numpy as np
from sklearn.metrics import accuracy_score, f1_score
import warnings
warnings.filterwarnings('ignore')

class BERTTeachingDemo:
    def __init__(self):
        """åˆå§‹åŒ–BERTæ•™å­¦æ¼”ç¤ºç±»"""
        self.device = torch.device('mps' if torch.mps.is_available() else 'cpu')
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")

    def load_bert_model(self):
        """1. åŠ è½½é¢„è®­ç»ƒBERTæ¨¡å‹"""
        print("=== 1. åŠ è½½BERTæ¨¡å‹ ===")

        try:
            # åŠ è½½tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained('bert-base-chinese')

            # åŠ è½½BERTæ¨¡å‹
            self.bert_model = BertModel.from_pretrained('bert-base-chinese').to(self.device)

            # åŠ è½½MLMä»»åŠ¡æ¨¡å‹
            self.mlm_model = BertForMaskedLM.from_pretrained('bert-base-chinese').to(self.device)

            print("âœ“ BERTæ¨¡å‹åŠ è½½å®Œæˆ")
            print(f"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in self.bert_model.parameters()):,}")
            return True

        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            print("ğŸ’¡ è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ä¸‹è½½æ¨¡å‹æ–‡ä»¶åˆ°æœ¬åœ°")
            print("   ç¦»çº¿æ¨¡å¼: è®¾ç½®ç¯å¢ƒå˜é‡ HF_HUB_OFFLINE=1")
            print("   æˆ–æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹åˆ°: ~/.cache/huggingface/hub/")
            return False

    def demonstrate_tokenization(self):
        """2. æ¼”ç¤ºBERTåˆ†è¯"""
        print("\n=== 2. BERTåˆ†è¯æ¼”ç¤º ===")

        text = "è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯æ­£åœ¨å¿«é€Ÿå‘å±•"

        # åŸºæœ¬åˆ†è¯
        tokens = self.tokenizer.tokenize(text)
        print(f"åŸå§‹æ–‡æœ¬: {text}")
        print(f"åˆ†è¯ç»“æœ: {tokens}")

        # è½¬æ¢ä¸ºID
        input_ids = self.tokenizer.encode(text, add_special_tokens=True)
        print(f"è¾“å…¥ID: {input_ids}")
        print(f"ç‰¹æ®Štoken: [CLS]={self.tokenizer.cls_token_id}, [SEP]={self.tokenizer.sep_token_id}")

    def demonstrate_attention_mask(self):
        """3. æ¼”ç¤ºæ³¨æ„åŠ›æ©ç """
        print("\n=== 3. æ³¨æ„åŠ›æ©ç æ¼”ç¤º ===")

        texts = ["ä»Šå¤©å¤©æ°”å¾ˆå¥½", "è‡ªç„¶è¯­è¨€å¤„ç†å¾ˆæœ‰è¶£"]
        encoded = self.tokenizer(texts, padding=True, truncation=True, return_tensors='pt')

        print("æ‰¹é‡ç¼–ç ç»“æœ:")
        print(f"è¾“å…¥IDå½¢çŠ¶: {encoded['input_ids'].shape}")
        print(f"æ³¨æ„åŠ›æ©ç : {encoded['attention_mask']}")

    def demonstrate_bert_forward(self):
        """4. æ¼”ç¤ºBERTå‰å‘ä¼ æ’­"""
        print("\n=== 4. BERTå‰å‘ä¼ æ’­æ¼”ç¤º ===")

        text = "BERTæ¨¡å‹èƒ½å¤Ÿå­¦ä¹ ä¸°å¯Œçš„è¯­è¨€è¡¨å¾"
        inputs = self.tokenizer(text, return_tensors='pt').to(self.device)

        with torch.no_grad():
            outputs = self.bert_model(**inputs)

        print(f"è¾“å…¥æ–‡æœ¬: {text}")
        print(f"last_hidden_stateå½¢çŠ¶: {outputs.last_hidden_state.shape}")
        print(f"pooler_outputå½¢çŠ¶: {outputs.pooler_output.shape}")

        # [CLS] tokençš„è¡¨å¾
        cls_embedding = outputs.last_hidden_state[0, 0, :]
        print(f"[CLS] tokenè¡¨å¾ç»´åº¦: {cls_embedding.shape}")

    def demonstrate_mlm_task(self):
        """5. æ¼”ç¤ºæ©ç è¯­è¨€æ¨¡å‹ä»»åŠ¡"""
        print("\n=== 5. æ©ç è¯­è¨€æ¨¡å‹(MLM)æ¼”ç¤º ===")

        text = "è‡ªç„¶è¯­è¨€å¤„ç†æ˜¯äººå·¥æ™ºèƒ½çš„é‡è¦ç ”ç©¶æ–¹å‘"
        print(f"åŸå§‹æ–‡æœ¬: {text}")

        # ä½¿ç”¨tokenizerè¿›è¡Œåˆ†è¯ï¼Œç„¶ååˆ›å»ºæ©ç 
        tokens = self.tokenizer.tokenize(text)
        print(f"åˆ†è¯ç»“æœ: {tokens}")
        
        # åˆ›å»ºæ©ç è¾“å…¥
        masked_tokens = tokens.copy()
        mask_positions = []

        # éšæœºæ©ç 15%çš„tokenï¼ˆè·³è¿‡ç‰¹æ®Štokenï¼‰
        for i, token in enumerate(tokens):
            if token not in ['[CLS]', '[SEP]'] and np.random.random() < 0.15:
                masked_tokens[i] = "[MASK]"
                mask_positions.append(i)

        # é‡æ–°ç¼–ç ä¸ºå®Œæ•´å¥å­
        masked_sentence = self.tokenizer.convert_tokens_to_string(masked_tokens)
        print(f"æ©ç å: {masked_sentence}")
        print(f"æ©ç ä½ç½®: {mask_positions}")

        # æ¨¡å‹é¢„æµ‹
        inputs = self.tokenizer(masked_sentence, return_tensors='pt').to(self.device)

        with torch.no_grad():
            outputs = self.mlm_model(**inputs)
            predictions = outputs.logits

        # è·å–é¢„æµ‹ç»“æœ
        mask_token_index = torch.where(inputs['input_ids'] == self.tokenizer.mask_token_id)[1]

        if len(mask_token_index) > 0:
            for i, pos in enumerate(mask_token_index):
                predicted_token_id = predictions[0, pos].argmax().item()
                predicted_token = self.tokenizer.decode(predicted_token_id)
                print(f"ä½ç½®{pos.item()}: [MASK] -> {predicted_token}")
        else:
            print("æœ¬æ¬¡è¿è¡Œæ²¡æœ‰ç”Ÿæˆæ©ç tokenï¼Œè¿™æ˜¯æ­£å¸¸çš„éšæœºè¡Œä¸º")

    def create_classification_dataset(self):
        """åˆ›å»ºæ–‡æœ¬åˆ†ç±»æ•°æ®é›†"""
        # æ¨¡æ‹Ÿæ–°é—»åˆ†ç±»æ•°æ®é›†
        texts = [
            "ç§‘æŠ€å…¬å¸å‘å¸ƒæ–°æ¬¾æ™ºèƒ½æ‰‹æœº",  # ç§‘æŠ€
            "æ”¿åºœå‡ºå°æ–°çš„ç»æµæ”¿ç­–",      # æ”¿æ²»
            "è¶³çƒæ¯”èµ›ç²¾å½©çº·å‘ˆ",          # ä½“è‚²
            "ç”µå½±ç¥¨æˆ¿åˆ›ä¸‹æ–°é«˜",          # å¨±ä¹
            "è‚¡å¸‚å‡ºç°å¤§å¹…æ³¢åŠ¨",          # è´¢ç»
        ]
        labels = [0, 1, 2, 3, 4]  # å¯¹åº”çš„æ ‡ç­¾

        return texts, labels

    def fine_tune_bert_classifier(self):
        """6. BERTå¾®è°ƒå®æˆ˜ï¼šæ–‡æœ¬åˆ†ç±»ï¼ˆå«éªŒè¯ç¯èŠ‚ï¼‰"""
        print("\n=== 6. BERTå¾®è°ƒå®æˆ˜ï¼šæ–°é—»åˆ†ç±» ===")

        # å‡†å¤‡è®­ç»ƒå’ŒéªŒè¯æ•°æ®
        train_texts, train_labels = self.create_classification_dataset()
        val_texts, val_labels = self.create_validation_dataset()  # æ–°å¢éªŒè¯é›†

        train_labels = torch.tensor(train_labels).to(self.device)
        val_labels = torch.tensor(val_labels).to(self.device)

        # åŠ è½½åˆ†ç±»æ¨¡å‹
        classifier = BertForSequenceClassification.from_pretrained(
            'bert-base-chinese',
            num_labels=5
        ).to(self.device)

        # ç¼–ç æ•°æ®
        train_inputs = self.tokenizer(train_texts, padding=True, truncation=True,
                                      return_tensors='pt').to(self.device)
        val_inputs = self.tokenizer(val_texts, padding=True, truncation=True,
                                    return_tensors='pt').to(self.device)  # ç¼–ç éªŒè¯æ•°æ®

        # è®­ç»ƒå‚æ•°
        optimizer = AdamW(classifier.parameters(), lr=2e-5)
        num_epochs = 5
        best_val_acc = 0.0  # è®°å½•æœ€ä½³éªŒè¯å‡†ç¡®ç‡

        print("å¼€å§‹å¾®è°ƒè®­ç»ƒ...")

        for epoch in range(num_epochs):
            # è®­ç»ƒé˜¶æ®µ
            classifier.train()
            optimizer.zero_grad()

            # å‰å‘ä¼ æ’­
            train_outputs = classifier(**train_inputs, labels=train_labels)
            train_loss = train_outputs.loss
            train_logits = train_outputs.logits

            # åå‘ä¼ æ’­
            train_loss.backward()
            optimizer.step()

            # è®¡ç®—è®­ç»ƒé›†å‡†ç¡®ç‡
            train_predictions = torch.argmax(train_logits, dim=1)
            train_accuracy = (train_predictions == train_labels).float().mean()

            # éªŒè¯é˜¶æ®µ
            classifier.eval()  # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼
            with torch.no_grad():  # å…³é—­æ¢¯åº¦è®¡ç®—
                val_outputs = classifier(**val_inputs, labels=val_labels)
                val_loss = val_outputs.loss
                val_logits = val_outputs.logits

                # è®¡ç®—éªŒè¯é›†æŒ‡æ ‡
                val_predictions = torch.argmax(val_logits, dim=1)
                val_accuracy = (val_predictions == val_labels).float().mean()

                # è®¡ç®—F1åˆ†æ•°ï¼ˆéœ€è¦è½¬æ¢ä¸ºnumpyæ•°ç»„ï¼‰
                val_f1 = f1_score(
                    val_labels.cpu().numpy(),
                    val_predictions.cpu().numpy(),
                    average='weighted'
                )

            print(f"Epoch {epoch + 1}/{num_epochs}")
            print(f"  è®­ç»ƒ: Loss: {train_loss.item():.3f}, Accuracy: {train_accuracy.item():.3f}")
            print(f"  éªŒè¯: Loss: {val_loss.item():.3f}, Accuracy: {val_accuracy.item():.3f}, F1: {val_f1:.3f}")

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_accuracy > best_val_acc:
                best_val_acc = val_accuracy
                torch.save(classifier.state_dict(), "best_bert_classifier.pt")
                print(f"  ä¿å­˜æœ€ä½³æ¨¡å‹ (éªŒè¯å‡†ç¡®ç‡: {best_val_acc.item():.3f})")

        print("âœ“ å¾®è°ƒå®Œæˆ")

        #å°†å¾®è°ƒå®Œæˆåçš„æ¨¡å‹.è¿›è¡ŒéªŒè¯
        # æœ€ç»ˆè¯„ä¼°å±•ç¤º
        self.evaluate_final_model(classifier, val_inputs, val_labels, val_texts)

    def create_validation_dataset(self):
        """åˆ›å»ºéªŒè¯æ•°æ®é›†"""
        # ä¸è®­ç»ƒé›†ä¸åŒçš„éªŒè¯æ ·æœ¬
        val_texts = [
            "äººå·¥æ™ºèƒ½æŠ€æœ¯å–å¾—é‡å¤§çªç ´",  # ç§‘æŠ€
            "è®®ä¼šé€šè¿‡æ–°çš„ç¯ä¿æ³•æ¡ˆ",  # æ”¿æ²»
            "ç¯®çƒé”¦æ ‡èµ›å† å†›è¯ç”Ÿ",  # ä½“è‚²
            "æ–°ä¸Šæ˜ ç”µå½±è·å¾—è§‚ä¼—å¥½è¯„",  # å¨±ä¹
            "å¤®è¡Œè°ƒæ•´é‡‘èæ”¿ç­–åº”å¯¹é€šèƒ€",  # è´¢ç»
        ]
        val_labels = [0, 1, 2, 3, 4]  # å¯¹åº”çš„æ ‡ç­¾
        return val_texts, val_labels

    def evaluate_final_model(self, model, val_inputs, val_labels, val_texts):
        """æœ€ç»ˆæ¨¡å‹è¯„ä¼°ä¸é¢„æµ‹æ¼”ç¤º"""
        print("\n=== æœ€ç»ˆæ¨¡å‹è¯„ä¼° ===")

        # åŠ è½½æœ€ä½³æ¨¡å‹
        model.load_state_dict(torch.load("best_bert_classifier.pt"))
        model.eval()

        # åœ¨éªŒè¯é›†ä¸Šè¿›è¡Œé¢„æµ‹
        with torch.no_grad():
            outputs = model(**val_inputs)
            predictions = torch.argmax(outputs.logits, dim=1)

        # æ ‡ç­¾æ˜ å°„
        label_map = {0: "ç§‘æŠ€", 1: "æ”¿æ²»", 2: "ä½“è‚²", 3: "å¨±ä¹", 4: "è´¢ç»"}

        # å±•ç¤ºé¢„æµ‹ç»“æœ
        print("\né¢„æµ‹ç»“æœç¤ºä¾‹:")
        for text, true_label, pred_label in zip(val_texts, val_labels, predictions):
            true_label_name = label_map[true_label.item()]
            pred_label_name = label_map[pred_label.item()]
            status = "âœ“" if true_label == pred_label else "âœ—"
            print(f"{status} æ–‡æœ¬: {text}")
            print(f"   çœŸå®æ ‡ç­¾: {true_label_name}, é¢„æµ‹æ ‡ç­¾: {pred_label_name}\n")

        # è®¡ç®—æ€»ä½“æŒ‡æ ‡
        accuracy = accuracy_score(val_labels.cpu().numpy(), predictions.cpu().numpy())
        f1 = f1_score(val_labels.cpu().numpy(), predictions.cpu().numpy(), average='weighted')
        print(f"æœ€ç»ˆéªŒè¯é›†æ€§èƒ½:")
        print(f"å‡†ç¡®ç‡: {accuracy:.3f}")
        print(f"F1åˆ†æ•°: {f1:.3f}")

    def compare_bert_vs_gpt(self):
        """7. BERTä¸GPTå¯¹æ¯”åˆ†æ"""
        print("\n=== 7. BERT vs GPT å¯¹æ¯”åˆ†æ ===")

        comparison = {
            "æ¶æ„": {
                "BERT": "åŒå‘Transformerç¼–ç å™¨",
                "GPT": "å•å‘Transformerè§£ç å™¨"
            },
            "æ³¨æ„åŠ›æœºåˆ¶": {
                "BERT": "å…¨è¯å…³æ³¨ï¼ˆæ— æ©ç ï¼‰",
                "GPT": "æ©ç è‡ªæ³¨æ„åŠ›"
            },
            "ä¼˜åŠ¿åœºæ™¯": {
                "BERT": "æ–‡æœ¬ç†è§£ä»»åŠ¡",
                "GPT": "æ–‡æœ¬ç”Ÿæˆä»»åŠ¡"
            },
            "é¢„è®­ç»ƒä»»åŠ¡": {
                "BERT": "MLM + NSP",
                "GPT": "è‡ªå›å½’è¯­è¨€å»ºæ¨¡"
            },
            "å‚æ•°é‡çº§": {
                "BERT": "Baseç‰ˆ1.1äº¿å‚æ•°",
                "GPT": "GPT-3è¾¾1750äº¿å‚æ•°"
            }
        }

        print("BERTä¸GPTæ ¸å¿ƒå¯¹æ¯”:")
        for aspect, models in comparison.items():
            print(f"{aspect}:")
            print(f"  BERT: {models['BERT']}")
            print(f"  GPT: {models['GPT']}")
            print()

    def run_all_demos(self):
        """è¿è¡Œæ‰€æœ‰BERTæ•™å­¦æ¼”ç¤º"""
        print("ğŸš€ å¼€å§‹BERTæ•™å­¦æ¼”ç¤º\n")

        if not self.load_bert_model():
            print("\nâš ï¸  ç”±äºæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè·³è¿‡éœ€è¦æ¨¡å‹çš„æ¼”ç¤º")
            print("ğŸ“– æ‚¨å¯ä»¥æŸ¥çœ‹ä»£ç æ³¨é‡Šäº†è§£BERTçš„å·¥ä½œåŸç†")
            return

        self.demonstrate_tokenization()
        self.demonstrate_attention_mask()
        self.demonstrate_bert_forward()
        self.demonstrate_mlm_task()
        self.fine_tune_bert_classifier()
        self.compare_bert_vs_gpt()

        print("\nğŸ‰ BERTæ•™å­¦æ¼”ç¤ºå®Œæˆï¼")

# ä¸»ç¨‹åº
if __name__ == "__main__":
    demo = BERTTeachingDemo()
    demo.run_all_demos()
